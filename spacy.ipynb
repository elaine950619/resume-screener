{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufXD7bLmSMWt"
      },
      "source": [
        "# Resume Parsing with SpaCy\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, we will explore how to use **SpaCy**, a powerful NLP library in Python, to parse resumes. We'll focus on extracting key information like names, phone numbers, emails, and LinkedIn URLs.\n",
        "\n",
        "### What is SpaCy?\n",
        "SpaCy is an open-source library for advanced **Natural Language Processing (NLP)** in Python. It is designed specifically for production use and offers:\n",
        "- **Pre-trained Models**: SpaCy provides models trained on large datasets to identify entities like names, organizations, and dates.\n",
        "- **Efficiency and Speed**: Built with performance in mind, SpaCy is one of the fastest NLP libraries.\n",
        "- **Customizable Pipelines**: You can easily add or modify components like tokenizers, taggers, and entity recognizers to fit your specific needs.\n",
        "- **Wide Applications**: From text classification and sentiment analysis to information extraction and resume parsing, SpaCy is versatile and powerful.\n",
        "\n",
        "### Why Use SpaCy for Resume Parsing?\n",
        "- **Pre-trained NER models** for entity extraction\n",
        "- **Interactive and customizable** for specialized tasks like resume parsing\n",
        "- **Fast and efficient** processing for large datasets\n",
        "- **Easy integration** with other Python libraries and tools\n",
        "\n",
        "#### Helpful Resources:\n",
        "- [SpaCy Documentation](https://spacy.io/usage)\n",
        "- [Named Entity Recognition in SpaCy](https://spacy.io/usage/linguistic-features#named-entities)\n",
        "- [Custom Components in SpaCy Pipelines](https://spacy.io/usage/processing-pipelines)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "bvOGEg6vSMWw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfplumber in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (0.11.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pdfminer.six==20231228->pdfplumber) (44.0.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: spacy in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (2.2.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (75.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/elaine/Documents/MDST/env/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "# Install SpaCy if you haven't already\n",
        "!pip install pdfplumber\n",
        "!pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vNeiAiKXSMWy"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.matcher import Matcher\n",
        "import re\n",
        "import pdfplumber"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vt5VdasXSMWz"
      },
      "source": [
        "## Step 1: Load SpaCy Model\n",
        "We'll start by loading the small English model `en_core_web_sm`, which includes tokenization, POS tagging, and NER."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "eNzClw9fSMW0"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6AbDgeZSMW0"
      },
      "source": [
        "## Step 2: Extract Text from a Resume PDF\n",
        "Use `pdfplumber` to extract text from the resume PDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D7KUXAXpSMW1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jake Ryan\n",
            "123-456-7890 | jake@su.edu | linkedin.com/in/jake | github.com/jake\n",
            "Education\n",
            "Southwestern University Georgetown, TX\n",
            "Bachelor of Arts in Computer Science, Minor in Business Aug. 2018 – May 2021\n",
            "Blinn College Bryan, TX\n",
            "Associate’s in Liberal Arts Aug. 2014 – May 2018\n",
            "Experience\n",
            "Undergraduate Research Assistant June 2020 – Present\n",
            "Texas A&M University College Station, TX\n",
            "• Developed a REST API using FastAPI and PostgreSQL to store data from learning management systems\n",
            "• Developed a full-\n"
          ]
        }
      ],
      "source": [
        "with pdfplumber.open(\"jakes-resume.pdf\") as pdf:\n",
        "    resume_text = \"\\n\".join([page.extract_text() for page in pdf.pages if page.extract_text()])\n",
        "\n",
        "print(resume_text[:500])  # Displaying the first 500 characters of the resume"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqTICvjSMW2"
      },
      "source": [
        "## Step 3: Named Entity Recognition (NER)\n",
        "SpaCy's NER can recognize entities like names, organizations, and more. Let's see what it can extract from the resume.\n",
        "\n",
        "### Exercise:\n",
        "After running the code, try highlighting specific entities (like `ORG`, `PERSON`, or `GPE`) to see how well SpaCy detects them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "32YjK6VBSMW2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PERSON: Jake Ryan\n",
            "CARDINAL: 123\n",
            "ORG: TX\n",
            "DATE: Aug. 2018\n",
            "DATE: May 2021\n",
            "PERSON: Blinn College Bryan\n",
            "ORG: TX\n",
            "WORK_OF_ART: Associate’s\n",
            "GPE: Liberal Arts Aug. 2014\n",
            "DATE: May 2018\n",
            "DATE: June 2020\n",
            "ORG: Texas A&M University College Station\n",
            "ORG: TX\n",
            "GPE: PostgreSQL\n",
            "PERSON: • Developed\n",
            "GPE: Flask\n",
            "GPE: React\n",
            "GPE: PostgreSQL\n",
            "PERSON: Docker\n",
            "ORG: GitHub\n",
            "ORG: • Explored\n",
            "PRODUCT: GitHub\n",
            "ORG: Information Technology Support Specialist\n",
            "DATE: Sep. 2018\n",
            "ORG: Southwestern University Georgetown\n",
            "ORG: TX\n",
            "ORG: • Assess\n",
            "CARDINAL: 200\n",
            "ORG: Artificial Intelligence Research\n",
            "DATE: May 2019\n",
            "DATE: July 2019\n",
            "ORG: Southwestern University Georgetown\n",
            "ORG: TX\n",
            "WORK_OF_ART: The Legend of Zelda\n",
            "• Developed\n",
            "PERSON: Java\n",
            "ORG: • Contributed\n",
            "CARDINAL: 50K+\n",
            "ORG: Git\n",
            "• Conducted\n",
            "ORG: • Wrote\n",
            "CARDINAL: 8\n",
            "ORG: • Presented\n",
            "ORG: the World Conference on Computational Intelligence\n",
            "Projects\n",
            "Gitlytics\n",
            "GPE: Flask\n",
            "GPE: React\n",
            "GPE: PostgreSQL\n",
            "DATE: June 2020\n",
            "GPE: Flask\n",
            "GPE: React\n",
            "PERSON: • Implemented\n",
            "WORK_OF_ART: • Used Celery\n",
            "PERSON: Redis\n",
            "PERSON: Simple Paintball\n",
            "PERSON: Spigot API\n",
            "PERSON: Java\n",
            "GPE: Maven\n",
            "PERSON: Git May 2018\n",
            "DATE: May 2020\n",
            "PRODUCT: Minecraft\n",
            "CARDINAL: 2K+\n",
            "CARDINAL: 4.5/5\n",
            "ORG: • Implemented\n",
            "ORG: Technical Skills\n",
            "Languages\n",
            "ORG: C/C++\n",
            "ORG: JavaScript\n",
            "ORG: HTML/CSS\n",
            "WORK_OF_ART: Frameworks: React\n",
            "GPE: Node.js\n",
            "GPE: Flask\n",
            "ORG: JUnit\n",
            "ORG: WordPress\n",
            "ORG: Material-UI\n",
            "PERSON: Docker\n",
            "PERSON: Cloud Platform\n",
            "PERSON: Visual Studio\n",
            "PERSON: PyCharm\n",
            "GPE: NumPy\n",
            "PERSON: Matplotlib\n"
          ]
        }
      ],
      "source": [
        "doc = nlp(resume_text)\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(f\"{ent.label_}: {ent.text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P39GjhhSMW3"
      },
      "source": [
        "## Step 4: Custom Pattern Matching with SpaCy's Matcher\n",
        "For entities like phone numbers, emails, and LinkedIn URLs, we can create custom patterns using `Matcher`.\n",
        "\n",
        "### Example Patterns:\n",
        "- **Phone Number**: Sequence of digits and optional symbols like `(`, `)`, `-`, and spaces.\n",
        "- **Email**: Text patterns with `@` symbol.\n",
        "- **LinkedIn URL**: URLs containing \"linkedin.com/in/\".\n",
        "\n",
        "### Exercise:\n",
        "Try tweaking the patterns to see if you can improve the detection of phone numbers or LinkedIn URLs. (if they don't work)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KhOg3Q5RSMW3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Phones: ['123-456-7890']\n",
            "Extracted Emails: ['jake@su.edu']\n",
            "Extracted LinkedIn URLs: ['https://linkedin.com/linkedin.com/in/jake']\n"
          ]
        }
      ],
      "source": [
        "# Regex pattern for phone numbers\n",
        "phone_regex = r'(\\+\\d{1,2}\\s)?(\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4})'\n",
        "phones = re.findall(phone_regex, resume_text)\n",
        "\n",
        "# Regex pattern for LinkedIn URLs\n",
        "linkedin_regex = r'linkedin\\.com/in/[A-Za-z0-9_\\-]+/?'\n",
        "linkedins = re.findall(linkedin_regex, resume_text)\n",
        "\n",
        "# Extract emails using SpaCy Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "email_pattern = [\n",
        "    {\"TEXT\": {\"REGEX\": r\"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\"}}\n",
        "]\n",
        "matcher.add(\"EMAIL\", [email_pattern])\n",
        "\n",
        "matches = matcher(doc)\n",
        "\n",
        "emails = []\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    emails.append(span.text)\n",
        "\n",
        "print(f\"Extracted Phones: {[phone[1] for phone in phones]}\")\n",
        "print(f\"Extracted Emails: {emails}\")\n",
        "print(f\"Extracted LinkedIn URLs: {[f'https://linkedin.com/{linkedin}' for linkedin in linkedins]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUnhliHVSMW3"
      },
      "source": [
        "## Step 5: Extracting Name Using NER\n",
        "SpaCy's NER can often detect the candidate's name under the `PERSON` label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "1xKtVVq9SMW3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Name: Jake Ryan\n"
          ]
        }
      ],
      "source": [
        "name = \"\"\n",
        "for ent in doc.ents:\n",
        "    if ent.label_ == \"PERSON\":\n",
        "        name = ent.text\n",
        "        break\n",
        "\n",
        "print(f\"Extracted Name: {name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3R01vQoSMW4"
      },
      "source": [
        "## Step 6: Your Turn - Extract Other Entities!\n",
        "Now it's your turn to extract other entities. Try finding:\n",
        "1. **Organizations (ORG)**\n",
        "2. **Locations (GPE)**\n",
        "3. **Dates (DATE)**\n",
        "\n",
        "### Exercise:\n",
        "Modify the code below to extract these entities and see how SpaCy performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "4oOBxbz_SMW4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted Organizations: ['TX', 'TX', 'Texas A&M University College Station', 'TX', 'GitHub', '• Explored', 'Information Technology Support Specialist', 'Southwestern University Georgetown', 'TX', '• Assess', 'Artificial Intelligence Research', 'Southwestern University Georgetown', 'TX', '• Contributed', 'Git\\n• Conducted', '• Wrote', '• Presented', 'the World Conference on Computational Intelligence\\nProjects\\nGitlytics', '• Implemented', 'Technical Skills\\nLanguages', 'C/C++', 'JavaScript', 'HTML/CSS', 'JUnit', 'WordPress', 'Material-UI']\n",
            "Extracted Locations: ['Liberal Arts Aug. 2014', 'PostgreSQL', 'Flask', 'React', 'PostgreSQL', 'Flask', 'React', 'PostgreSQL', 'Flask', 'React', 'Maven', 'Node.js', 'Flask', 'NumPy']\n",
            "Extracted Dates: ['Aug. 2018', 'May 2021', 'May 2018', 'June 2020', 'Sep. 2018', 'May 2019', 'July 2019', 'June 2020', 'May 2020']\n"
          ]
        }
      ],
      "source": [
        "# Example for extracting organizations\n",
        "organizations = [ent.text for ent in doc.ents if ent.label_ == \"ORG\"]\n",
        "print(f\"Extracted Organizations: {organizations}\")\n",
        "\n",
        "locations = [ent.text for ent in doc.ents if ent.label_ == \"GPE\"]\n",
        "print(f\"Extracted Locations: {locations}\")\n",
        "\n",
        "dates = [ent.text for ent in doc.ents if ent.label_ == \"DATE\"]\n",
        "print(f\"Extracted Dates: {dates}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwMS78d_SMW4"
      },
      "source": [
        "## Step 7: Combining All Extracted Information\n",
        "We'll now compile all extracted details into a structured format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "0rK7e5qTSMW5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Name': 'Jake Ryan', 'Phone': '123-456-7890', 'Email': 'jake@su.edu', 'LinkedIn': 'https://linkedin.com/linkedin.com/in/jake'}\n"
          ]
        }
      ],
      "source": [
        "parsed_resume = {\n",
        "    \"Name\": name,\n",
        "    \"Phone\": phones[0][1],\n",
        "    \"Email\": emails[0],\n",
        "    \"LinkedIn\": f'https://linkedin.com/{linkedins[0]}'\n",
        "}\n",
        "\n",
        "for match_id, start, end in matches:\n",
        "    span = doc[start:end]\n",
        "    match_label = nlp.vocab.strings[match_id]\n",
        "    if match_label == \"EMAIL\":\n",
        "        parsed_resume[\"Email\"] = span.text\n",
        "\n",
        "print(parsed_resume)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME-yrDSnSMW5"
      },
      "source": [
        "## Final Thoughts\n",
        "- **SpaCy** offers powerful tools for both general and custom text extraction.\n",
        "- You can further enhance this by training a **custom NER model** for more specialized resume parsing.\n",
        "- Integrate this into larger applications to automate resume screening.\n",
        "\n",
        "Happy Parsing! 🚀"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
